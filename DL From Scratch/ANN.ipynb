{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6e5435",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "13dc22df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd10e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://datascience.stackexchange.com/questions/75855/what-types-of-matrix-multiplication-are-used-in-machine-learning-when-are-they\n",
    "# I wanna be able to stack layers\n",
    "# idea is if tthis get passed to antoher then it will build the neural net\n",
    "# build weight matrix\n",
    "# get frontprop calc to work\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sig = sigmoid(x)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def binary_cross_entropy(y_pred, y_true, epsilon=1e-12):\n",
    "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    loss = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return np.mean(loss)\n",
    "\n",
    "def binary_cross_entropy_derivative(y_pred, y_true, epsilon=1e-12):\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -(y_true / y_pred) + (1 - y_true) / (1 - y_pred)\n",
    "\n",
    "class LinearLayer():\n",
    "    def __init__(self, inFeature, outFeature, activationFunction=None) -> None:\n",
    "        self.W=np.random.randint(10, size=(outFeature, inFeature))\n",
    "        self.b = np.zeros((1, outFeature))\n",
    "        self.trainable=True\n",
    "        self.activation = activationFunction\n",
    "        self.dEdx=0\n",
    "        self.dEdy=0\n",
    "        self.y=0\n",
    "        # print(self.W)\n",
    "    def Forward(self, X):\n",
    "        output=X@self.W+self.b\n",
    "        # print(output)\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "        # print(output)\n",
    "\n",
    "        self.X = X \n",
    "        self.y=output\n",
    "        return output\n",
    "    def Backward(self, y, lr=0.01):\n",
    "        self.dEdy=binary_cross_entropy_derivative(self.y-y)\n",
    "        dydx=sigmoid_derivative(self.y)\n",
    "        self.dEdx= dydx*self.dEdy\n",
    "        dEdW=self.X.T@self.dEdx\n",
    "        dEdb=self.dEdx.sum()\n",
    "        # find out how the weight and bias affect the error\n",
    "        self.W-=lr*dEdW\n",
    "        self.b-=lr*dEdb\n",
    "        return \n",
    "    def Reset():\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1436db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: {epoch}\n",
      "current epoch: {epoch}\n",
      "current epoch: {epoch}\n",
      "current epoch: {epoch}\n",
      "current epoch: {epoch}\n",
      "current epoch: {epoch}\n",
      "current epoch: {epoch}\n",
      "current epoch: {epoch}\n",
      "current epoch: {epoch}\n",
      "current epoch: {epoch}\n"
     ]
    }
   ],
   "source": [
    "X=np.random.randint(10, size=(1,3))\n",
    "y=softmax(np.random.randint(10, size=(1,10)))\n",
    "# print(X)\n",
    "\n",
    "layerSequence=[\n",
    "LinearLayer(4,3, sigmoid)\n",
    ",LinearLayer(10,4, sigmoid)\n",
    ",LinearLayer(5,10, sigmoid)\n",
    "# below this is the output layer\n",
    ",LinearLayer(10, 5, sigmoid)\n",
    "]\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, layers) -> None:\n",
    "        self.layers=layers\n",
    "    def ForwardProp(X):\n",
    "        for i in range(len(layerSequence)):\n",
    "            X=layerSequence[i].Forward(X)\n",
    "        return X\n",
    "\n",
    "# print(y_logits)\n",
    "# print(y)\n",
    "# print(binary_cross_entropy(y_logits,y))\n",
    "# print(y_logits-y)\n",
    "model=Model(layers=layerSequence)\n",
    "epochs=10   \n",
    "for epoch in range(epochs):\n",
    "    print(\"current epoch: {epoch}\")\n",
    "    y_logits = model.ForwardProp(X)\n",
    "    y_pred = softmax(y_logits) \n",
    "    loss = binary_cross_entropy(y_logits, y) \n",
    "    \n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
